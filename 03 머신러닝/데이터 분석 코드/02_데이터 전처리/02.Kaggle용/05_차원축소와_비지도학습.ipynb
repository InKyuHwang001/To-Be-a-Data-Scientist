{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.주성분 분석(PCA)"
      ],
      "metadata": {
        "id": "JzPXqfSSe-73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 다차원 데이터 분산이 큰 방향에서부터 순서대로 축을 다시 잡는 방법이다.\n",
        "- 변수간 종속성이 클 수록 원래 데이터를 잘 표현한다.\n",
        "- 단 **정규분포를 가정** 하기에 외도가 큰 경우 문제다.\n"
      ],
      "metadata": {
        "id": "aAPiyejZ2fi7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w845Gznbd3kq"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 데이터는 표준화 등의 스케일을 갖추기 위한 전처리가 이루어져야 함\n",
        "\n",
        "# 학습 데이터를 기반으로 PCA에 의한 변환을 정의\n",
        "pca = PCA(n_components=5)\n",
        "pca.fit(train_x)\n",
        "\n",
        "# 변환 적용\n",
        "train_x = pca.transform(train_x)\n",
        "test_x = pca.transform(test_x)\n",
        "\n",
        "# -----------------------------------\n",
        "# 표준화된 데이터를 사용\n",
        "train_x, test_x = load_standarized_data()\n",
        "# -----------------------------------\n",
        "# TruncatedSVD\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# 데이터는 표준화 등의 스케일을 갖추기 위한 전처리가 이루어져야 함\n",
        "\n",
        "# 학습 데이터를 기반으로 SVD를 통한 변환 정의\n",
        "svd = TruncatedSVD(n_components=5, random_state=71)\n",
        "svd.fit(train_x)\n",
        "\n",
        "# 변환 적용\n",
        "train_x = svd.transform(train_x)\n",
        "test_x = svd.transform(test_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.음수 미포함 행렬 분해"
      ],
      "metadata": {
        "id": "F0AXktUz-xn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x = load_minmax_scaled_data()\n",
        "# -----------------------------------\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# 데이터는 음수가 아닌 값으로 구성\n",
        "\n",
        "# 학습 데이터를 기반으로 NMF에 의한 변환 정의\n",
        "model = NMF(n_components=5, init='random', random_state=71)\n",
        "model.fit(train_x)\n",
        "\n",
        "# 변환 적용\n",
        "train_x = model.transform(train_x)\n",
        "test_x = model.transform(test_x)"
      ],
      "metadata": {
        "id": "xFOOHqDH-2kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.잠재 디리클레 할당(LDA)"
      ],
      "metadata": {
        "id": "9EFM3rNyANgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- nlp 토픽모델링에서 사용됨\n"
      ],
      "metadata": {
        "id": "4XTvfyATCVAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x = load_minmax_scaled_data()\n",
        "# -----------------------------------\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# 데이터는 단어-문서의 카운트 행렬 등으로 함\n",
        "\n",
        "# 학습 데이터를 기반으로 LDA에 의한 변환을 정의\n",
        "model = LatentDirichletAllocation(n_components=5, random_state=71)\n",
        "model.fit(train_x)\n",
        "\n",
        "# 변환 적용\n",
        "train_x = model.transform(train_x)\n",
        "test_x = model.transform(test_x)"
      ],
      "metadata": {
        "id": "MY1E5vezANzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.선형판별분석(LDA)"
      ],
      "metadata": {
        "id": "VU611s6kCs-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "n행의 데이터에f개의 틱징으로 이루어진 n\\*f 행렬에 f\\*k를 곱해 만든 n\\*k행렬"
      ],
      "metadata": {
        "id": "spPqG9DmC2Ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x = load_standarized_data()\n",
        "# -----------------------------------\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "\n",
        "# 데이터는 단어-문서의 카운트 행렬 등으로 함\n",
        "\n",
        "# 학습 데이터를 기반으로 LDA에 의한 변환을 정의\n",
        "lda = LDA(n_components=1)\n",
        "lda.fit(train_x, train_y)\n",
        "\n",
        "# 변환 적용\n",
        "train_x = lda.transform(train_x)\n",
        "test_x = lda.transform(test_x)"
      ],
      "metadata": {
        "id": "XuqOYGutDP0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.t-SNE"
      ],
      "metadata": {
        "id": "LrXEEitWDQ-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터를 2차원 평면상에 압축하여 시각화 목적으로"
      ],
      "metadata": {
        "id": "bGnf-rC4DT54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x = load_standarized_data()\n",
        "# -----------------------------------\n",
        "import bhtsne\n",
        "\n",
        "# 데이터는 표준화 등의 스케일을 갖추기 위한 전처리가 이루어져야 함\n",
        "\n",
        "# t-sne에 의한 변환\n",
        "data = pd.concat([train_x, test_x])\n",
        "embedded = bhtsne.tsne(data.astype(np.float64), dimensions=2, rand_seed=71)"
      ],
      "metadata": {
        "id": "JPL_cb6LDUP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.UMAP"
      ],
      "metadata": {
        "id": "BeVBuxk3Dkfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE 보다 빠른 방식"
      ],
      "metadata": {
        "id": "i-Tz4FFhDpia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x = load_standarized_data()\n",
        "# -----------------------------------\n",
        "import umap\n",
        "\n",
        "# 데이터는 표준화 등의 스케일을 갖추는 전처리가 이루어져야 함\n",
        "\n",
        "# 학습 데이터를 기반으로 UMAP에 의한 변환을 정의\n",
        "um = umap.UMAP()\n",
        "um.fit(train_x)\n",
        "\n",
        "# 변환 적용\n",
        "train_x = um.transform(train_x)\n",
        "test_x = um.transform(test_x)"
      ],
      "metadata": {
        "id": "LDz7FEIBEZbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.군집화"
      ],
      "metadata": {
        "id": "n_cjeN3EEcLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 여러 구릅으로 나누는 비지도 학습\n",
        "- K-Means or Mini-Batch K-Means\n",
        "- DBSCAN\n",
        "- 병합군집: 응집형 계층 클러스터링"
      ],
      "metadata": {
        "id": "e6Iqqe8YEpOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x = load_standarized_data()\n",
        "# -----------------------------------\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "# 데이터는 표준화 등의 스케일을 갖추는 전처리가 이루어져야 함\n",
        "\n",
        "# 학습 데이터를 기반으로 Mini-Batch K-Means를 통한 변환 정의\n",
        "kmeans = MiniBatchKMeans(n_clusters=10, random_state=71)\n",
        "kmeans.fit(train_x)\n",
        "\n",
        "# 해당 클러스터를 예측\n",
        "train_clusters = kmeans.predict(train_x)\n",
        "test_clusters = kmeans.predict(test_x)\n",
        "\n",
        "# 각 클러스터 중심까지의 거리를 저장\n",
        "train_distances = kmeans.transform(train_x)\n",
        "test_distances = kmeans.transform(test_x)"
      ],
      "metadata": {
        "id": "DzuvRfE_FAhi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}